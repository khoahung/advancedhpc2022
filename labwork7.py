# -*- coding: utf-8 -*-
"""labwork7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GBmdAuQXenqmu0ZUoORDo9d8YQfH53rq
"""

import sys
import numba
from numba import cuda
import numpy as np
from numba import vectorize
import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from numpy import asarray


img = cv.imread("example-orig.jpg")

im_resized = cv.resize(img, (224, 224), interpolation=cv.INTER_LINEAR)
plt.imshow(cv.cvtColor(im_resized, cv.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

x_data = np.array(img)
flatSrc = x_data.flatten()

imageWidth,imageHeight,c = x_data.shape
print(x_data.shape)

@cuda.jit
def sum1D(src, dst, sharedSize):
      # shared memory declaration for caching block content cache = cuda.shared.array((sharedSize, ), np.float32) localtid = threadIdx.x
      tid = threadIdx.x + blockIdx.x * blockDim.x
      # copy local block from src to cache (in shared memory) cache[localtid] = src[tid]
      cuda.syncthreads()
      # reduction in cache
      s=1
      while s < blockDim.x:
        if localtid % (s * 2) == 0: 
              cache[tid] += cache[tid + s]
        cuda.syncthreads()
        s=s*2
      # only first thread writes back to dst
      if localtid == 0: dst[blockIdx.x] = cache[0]


pixelCount = imageWidth * imageHeight

devSrc = cuda.to_device(x_data)
devDst = cuda.device_array((pixelCount, 3), np.uint8)
sum1D(devSrc,devDst,64)